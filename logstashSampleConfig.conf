# This starts the configuration for data coming into Logstash
input {
	
	# Input to Logstash will come via TCP/IP connections
	tcp {
		
		# Mode should be set to server;  This spins up an independent Logstash server to 
		# listen for communications to clean, collect, and forward into the output
		mode => "server"
		
		# This is the IP Address used for the demo.  The 192.168 IP range is another 
		# localhost mapping, but is needed to make sure the Shiny application is 
		# pointed at the correct IP and port addresses
		host => "192.168.1.1"
	
		# Using my birthday as the port number and on OSX it shouldn't be used by other 
		# processes and should be safe generally for regular use
		port => 6983
	
		# The codec tells Logstash what type of data to expect so it can parse it correctly
		codec => "json"
	
	} # This is the end of the TCP input listener configuration

} # This is the end of the configuration for all input sources

# Now we specify settings that tell Logstash how to filter/intpret the data further
filter {

	# For the demo it needs to know how to filter JSON data being piped in from the Shiny Application
	json {
		
		# This tells logstash what it needs to process;  The Shiny application 
		# pipes the JSON data into the TCP listener as a message.  Basically, 
		# this is the configuration that I found to work and it was easier/more 
		# stable to continue using this.
		source => "message"

	} # This ends the filter section to define what else to process from the input

} # This ends the filter configuration setup

# Now we need to configure where Logstash should send the data for storage
output {

	# We want to pipe the data into Elasticsearch so we can visualize it with Kibana later
	elasticsearch {

		# Logstash can spin up its own internal elasticsearch server, but you 
		# should have some separation between these in production; so set this 
		# value to false
		embedded => false

		# Elasticsearch is running on the local machine, so you can use 
		# localhost as the ip address
		host => "localhost"

		# For protocol http and transport the port address needs to point towards 
		# the HTTP port used by elasticsearch which defaults to 9200
		port => "9200"

		# Was using node protocol previously, but it had some issues with connection 
		# timeouts.  Seems to work much easier with http protocol
		protocol => "http"

		# This is the name of the elasticsearch cluster logstash should look for
		cluster => "sdpDemo"
	
		# This will be the index used to store the data it will create a separate index
		# for each day, but you can change this to any index name you would like.  
		# It might not be a bad idea to name your indices for the applications whose data 
		# they contain.
		index => "logstash-%{+YYYY.MM.dd}"

	} # This ends the configuration settings to send the data out to Elasticsearch

} # This ends the output configuration
